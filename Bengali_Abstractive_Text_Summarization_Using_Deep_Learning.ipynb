{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bengali Abstractive Text Summarization Using Deep Learning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbuKaisar24/Bengali-abstractive-text-summarization-using-sequence-to-sequence-RNNs/blob/master/Bengali_Abstractive_Text_Summarization_Using_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gC7bopBDDKZn",
        "colab_type": "code",
        "outputId": "0edc66da-614a-4164-8059-abe4985b051c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkZT-s4NDKZv",
        "colab_type": "code",
        "outputId": "20b7bda1-a13f-4b53-b393-464891acd23e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "df=pd.read_excel(\"gdrive/My Drive/Colab Notebooks/newdata.xlsx\",sep=\",\", iterator=True)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Post Type</th>\n",
              "      <th>Text</th>\n",
              "      <th>Summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>page</td>\n",
              "      <td>আগে যখন আমি ফুটবল বুঝতাম না ও দেখতাম না তখন ভা...</td>\n",
              "      <td>মেসি সবার সেরা।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>page</td>\n",
              "      <td>অগ্নিকান্ডের সময় আমাদের করণীয় তো আমরা সবাই জান...</td>\n",
              "      <td>অগ্নিকান্ডের সময় উপস্থিত জনগণের করণীয়।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>personal</td>\n",
              "      <td>ড্যাফোডিলের মেইন ক্যাম্পাসটারে ধানমন্ডি থেইকা ...</td>\n",
              "      <td>জ্যামের জন্য এক্সাম মিস ।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>page</td>\n",
              "      <td>পাইপ বয় নাঈমকে খালেদা জিয়ার বিরুদ্ধে কথাটি শিখ...</td>\n",
              "      <td>নাঈমকে টাকা দেয়া হচ্ছে ।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>personal</td>\n",
              "      <td>সাংবাদিক উজ্জ্বল সে একজন প্রতারক,সামন্য অর্থের...</td>\n",
              "      <td>উজ্জ্বল অর্থের প্রয়োজনে মিথ্যা সংবাদ প্রচার করে।</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Post Type  ...                                           Summary\n",
              "0      page  ...                                   মেসি সবার সেরা।\n",
              "1      page  ...           অগ্নিকান্ডের সময় উপস্থিত জনগণের করণীয়।\n",
              "2  personal  ...                         জ্যামের জন্য এক্সাম মিস ।\n",
              "3      page  ...                          নাঈমকে টাকা দেয়া হচ্ছে ।\n",
              "4  personal  ...  উজ্জ্বল অর্থের প্রয়োজনে মিথ্যা সংবাদ প্রচার করে।\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aihZrVb7DKZ5",
        "colab_type": "code",
        "outputId": "20e9f670-bcb0-4991-dc48-b46fec04a3d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bNQUDSiDKZ_",
        "colab_type": "code",
        "outputId": "930c3c35-f877-4835-fd39-268f399f1cb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Post Type    0\n",
              "Text         0\n",
              "Summary      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjsVtoqmDKaF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=df.drop(\"Post Type\",axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAFeR2dxDKaK",
        "colab_type": "code",
        "outputId": "c66af649-b07d-476f-b9cf-69819d92927f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>আগে যখন আমি ফুটবল বুঝতাম না ও দেখতাম না তখন ভা...</td>\n",
              "      <td>মেসি সবার সেরা।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>অগ্নিকান্ডের সময় আমাদের করণীয় তো আমরা সবাই জান...</td>\n",
              "      <td>অগ্নিকান্ডের সময় উপস্থিত জনগণের করণীয়।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ড্যাফোডিলের মেইন ক্যাম্পাসটারে ধানমন্ডি থেইকা ...</td>\n",
              "      <td>জ্যামের জন্য এক্সাম মিস ।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>পাইপ বয় নাঈমকে খালেদা জিয়ার বিরুদ্ধে কথাটি শিখ...</td>\n",
              "      <td>নাঈমকে টাকা দেয়া হচ্ছে ।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>সাংবাদিক উজ্জ্বল সে একজন প্রতারক,সামন্য অর্থের...</td>\n",
              "      <td>উজ্জ্বল অর্থের প্রয়োজনে মিথ্যা সংবাদ প্রচার করে।</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text                                           Summary\n",
              "0  আগে যখন আমি ফুটবল বুঝতাম না ও দেখতাম না তখন ভা...                                   মেসি সবার সেরা।\n",
              "1  অগ্নিকান্ডের সময় আমাদের করণীয় তো আমরা সবাই জান...           অগ্নিকান্ডের সময় উপস্থিত জনগণের করণীয়।\n",
              "2  ড্যাফোডিলের মেইন ক্যাম্পাসটারে ধানমন্ডি থেইকা ...                         জ্যামের জন্য এক্সাম মিস ।\n",
              "3  পাইপ বয় নাঈমকে খালেদা জিয়ার বিরুদ্ধে কথাটি শিখ...                          নাঈমকে টাকা দেয়া হচ্ছে ।\n",
              "4  সাংবাদিক উজ্জ্বল সে একজন প্রতারক,সামন্য অর্থের...  উজ্জ্বল অর্থের প্রয়োজনে মিথ্যা সংবাদ প্রচার করে।"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "1slFXdodDKaQ",
        "colab_type": "code",
        "outputId": "01f37e58-419d-4086-d060-ac7bb5a681d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "\n",
        "for i in range(5):\n",
        "    print(\"News:\",i+1)\n",
        "    print(\"Text:\",df.Text[i])\n",
        "    print(\"Summary:\",df.Summary[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "News: 1\n",
            "Text: আগে যখন আমি ফুটবল বুঝতাম না ও দেখতাম না তখন ভাবতাম মেসিই সেরা। তারপর যখন আমি ফুটবল বোঝা ও দেখা শুরু করলাম তখন উপলব্ধি করলাম যে,আগে আমি ভুল ছিলাম না।\n",
            "Summary: মেসি সবার সেরা।\n",
            "News: 2\n",
            "Text: অগ্নিকান্ডের সময় আমাদের করণীয় তো আমরা সবাই জানি। কিন্তু আমরা কি জানি, যেকোনো অগ্নিকান্ডের সময় দুর্ঘটনাস্থলে উপস্থিত থাকা সাধারণ জনগনের করণীয় কি? চলুন জেনে নেই দুর্ঘটনাস্থলে উপস্থিত থাকা সাধারণ জনগনের করণীয়গুলো। প্রয়োজনীয় কিছু পদক্ষেপ গ্রহন করে মূল্যবান জীবন বাঁচাই।\n",
            "Summary: অগ্নিকান্ডের সময় উপস্থিত জনগণের করণীয়।\n",
            "News: 3\n",
            "Text: ড্যাফোডিলের মেইন ক্যাম্পাসটারে ধানমন্ডি থেইকা গাবতলী ট্রান্সফারের জন্য একটা সিরিয়াস আন্দোলন দরকার।২ সেমিস্টার যাবত এই জ্যামের জন্য ৮ঃ৩০ টার একটা ক্লাস ও করতে পারি না...মজার রোড থেইক্কা কল্যাণপুর আসতেই লাগলো ২ ঘণ্টা...এতো সুন্দর জীবন দিয়া কি করবো যদি এক্সাম মিস হয়।\n",
            "Summary:  জ্যামের জন্য এক্সাম মিস ।\n",
            "News: 4\n",
            "Text: পাইপ বয় নাঈমকে খালেদা জিয়ার বিরুদ্ধে কথাটি শিখিয়ে দেয়া হয়েছে দাবী তার মায়ের, বললেন ৫০০০ ডলার পুরস্কার ঘোষনাকারী সামী। তবে নাঈম তার ভুল বুঝতে পারায় শেষ পর্যন্ত টাকা দেয়া হচ্ছে তাকে।\n",
            "Summary: নাঈমকে টাকা দেয়া হচ্ছে ।\n",
            "News: 5\n",
            "Text: সাংবাদিক উজ্জ্বল সে একজন প্রতারক,সামন্য অর্থের প্রয়োজনে সে মানুষের বিরুদ্বে বিমাতা স্বরুপ মিথ্যা ভুল সংবাদ প্রচার করে মানুষের সম্মান হানি করে এই ধরনের সাংবাদিকদের কারনে সমাজ আজ হুমকি মুখে মানুষকে বিপদগ্রস্ত করার জন্য সে সর্বদা লিপ্ত থাকে।\n",
            "Summary: উজ্জ্বল অর্থের প্রয়োজনে মিথ্যা সংবাদ প্রচার করে।\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVLf89o_DKaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contractions = { \n",
        "\"বি.দ্র \": \"বিশেষ দ্রষ্টব্য\",\n",
        "\"ড.\": \"ডক্টর\",\n",
        "\"ডা.\": \"ডাক্তার\",\n",
        "\"ইঞ্জি:\": \"ইঞ্জিনিয়ার\",\n",
        "\"রেজি:\": \"রেজিস্ট্রেশন\",\n",
        "\"মি.\": \"মিস্টার\",\n",
        "\"মু.\": \"মুহাম্মদ\",\n",
        "\"মো.\": \"মোহাম্মদ\",\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc-Rlo1xDKab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import string\n",
        "def clean_text(text,remove_stopwords = False):\n",
        "    if True:\n",
        "        text = text.split()\n",
        "        new_text = []\n",
        "        for word in text:\n",
        "            if word in contractions:\n",
        "                new_text.append(contractions[word])\n",
        "            else:\n",
        "                new_text.append(word)\n",
        "        text = \" \".join(new_text)\n",
        "    # Format words and remove unwanted characters\n",
        "    whitespace = re.compile(u\"[\\s\\u0020\\u00a0\\u1680\\u180e\\u202f\\u205f\\u3000\\u2000-\\u200a]+\", re.UNICODE)\n",
        "    bangla_digits = u\"[\\u09E6\\u09E7\\u09E8\\u09E9\\u09EA\\u09EB\\u09EC\\u09ED\\u09EE\\u09EF]+\"\n",
        "    english_chars = u\"[a-zA-Z0-9]\"\n",
        "    punc = u\"[(),$%^&*+={}\\[\\]:\\\"|\\'\\~`<>/,¦!?½£¶¼©⅐⅑⅒⅓⅔⅕⅖⅗⅘⅙⅚⅛⅜⅝⅞⅟↉¤¿º;-]+\"\n",
        "    bangla_fullstop = u\"\\u0964\"     #bangla fullstop(dari)\n",
        "    punctSeq   = u\"['\\\"“”‘’]+|[.?!,…]+|[:;]+\"\n",
        "    \n",
        "    text = re.sub(bangla_digits, \" \", text)\n",
        "    text = re.sub(punc, \" \", text)\n",
        "    text = re.sub(english_chars, \" \", text)\n",
        "    text = re.sub(bangla_fullstop, \" \", text)\n",
        "    text = re.sub(punctSeq, \" \", text)\n",
        "    text = whitespace.sub(\" \", text).strip()\n",
        "    \n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\<a href', ' ', text)\n",
        "    text = re.sub(r'&amp;‘:‘ ’', '', text) \n",
        "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]। ,', ' ', text)\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    text = re.sub(r'\\'', ' ', text)\n",
        "    text = re.sub(r\"[\\@$#%~+-\\.\\'।\\\"]\",\" \",text)\n",
        "    text = re.sub(r\"(?m)^\\s+\", \"\", text)\n",
        "    text = re.sub(\"[()]\",\"\",text)\n",
        "    text = re.sub(\"[‘’]\",\"\",text)\n",
        "    text = re.sub(\"[!]\",\"\",text)\n",
        "    text = re.sub(\"[/]\",\"\",text)\n",
        "    text = re.sub(\"[:]\",\"\",text)\n",
        "    text= re.sub('\\ |\\?|\\.|\\!|\\/|\\;|\\:', ' ',text)\n",
        "    text= text.strip(\"/\")\n",
        "    \n",
        "    if remove_stopwords:\n",
        "        k = []\n",
        "        with open('gdrive/My Drive/Colab Notebooks/Banglastopword.txt', 'r',encoding=\"utf-8\") as f:\n",
        "            for word in f:\n",
        "                word = word.split()\n",
        "                k.append(word[0])\n",
        "            text = [t for t in text if t not in k]\n",
        "            text = \"\".join(text)\n",
        "            \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0cmfWFcDKag",
        "colab_type": "code",
        "outputId": "634e8c62-a27d-4fb7-dea8-abd68785e002",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "clean_summaries = []\n",
        "for summary in df.Summary:\n",
        "    clean_summaries.append(clean_text(summary,remove_stopwords=False))\n",
        "print(\"Summaries are complete.\")\n",
        "\n",
        "clean_texts = []\n",
        "for text in df.Text:\n",
        "    clean_texts.append(clean_text(text))\n",
        "print(\"Texts are complete.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summaries are complete.\n",
            "Texts are complete.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "pP36C90NDKal",
        "colab_type": "code",
        "outputId": "9bdb75bb-e834-4a79-f2c5-346fbecc3787",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "for i in range(5):\n",
        "    print(\"Clean Text:#\",i+1)\n",
        "    print(\"Clean Summary:\",clean_summaries[i])\n",
        "    print(\"Clean Text:\",clean_texts[i])\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clean Text:# 1\n",
            "Clean Summary: মেসি সবার সেরা\n",
            "Clean Text: আগে যখন আমি ফুটবল বুঝতাম না ও দেখতাম না তখন ভাবতাম মেসিই সেরা তারপর যখন আমি ফুটবল বোঝা ও দেখা শুরু করলাম তখন উপলব্ধি করলাম যে আগে আমি ভুল ছিলাম না\n",
            "\n",
            "Clean Text:# 2\n",
            "Clean Summary: অগ্নিকান্ডের সময় উপস্থিত জনগণের করণীয়\n",
            "Clean Text: অগ্নিকান্ডের সময় আমাদের করণীয় তো আমরা সবাই জানি কিন্তু আমরা কি জানি যেকোনো অগ্নিকান্ডের সময় দুর্ঘটনাস্থলে উপস্থিত থাকা সাধারণ জনগনের করণীয় কি চলুন জেনে নেই দুর্ঘটনাস্থলে উপস্থিত থাকা সাধারণ জনগনের করণীয়গুলো প্রয়োজনীয় কিছু পদক্ষেপ গ্রহন করে মূল্যবান জীবন বাঁচাই\n",
            "\n",
            "Clean Text:# 3\n",
            "Clean Summary: জ্যামের জন্য এক্সাম মিস\n",
            "Clean Text: ড্যাফোডিলের মেইন ক্যাম্পাসটারে ধানমন্ডি থেইকা গাবতলী ট্রান্সফারের জন্য একটা সিরিয়াস আন্দোলন দরকার সেমিস্টার যাবত এই জ্যামের জন্য ঃ টার একটা ক্লাস ও করতে পারি না মজার রোড থেইক্কা কল্যাণপুর আসতেই লাগলো ঘণ্টা এতো সুন্দর জীবন দিয়া কি করবো যদি এক্সাম মিস হয়\n",
            "\n",
            "Clean Text:# 4\n",
            "Clean Summary: নাঈমকে টাকা দেয়া হচ্ছে\n",
            "Clean Text: পাইপ বয় নাঈমকে খালেদা জিয়ার বিরুদ্ধে কথাটি শিখিয়ে দেয়া হয়েছে দাবী তার মায়ের বললেন ডলার পুরস্কার ঘোষনাকারী সামী তবে নাঈম তার ভুল বুঝতে পারায় শেষ পর্যন্ত টাকা দেয়া হচ্ছে তাকে\n",
            "\n",
            "Clean Text:# 5\n",
            "Clean Summary: উজ্জ্বল অর্থের প্রয়োজনে মিথ্যা সংবাদ প্রচার করে\n",
            "Clean Text: সাংবাদিক উজ্জ্বল সে একজন প্রতারক সামন্য অর্থের প্রয়োজনে সে মানুষের বিরুদ্বে বিমাতা স্বরুপ মিথ্যা ভুল সংবাদ প্রচার করে মানুষের সম্মান হানি করে এই ধরনের সাংবাদিকদের কারনে সমাজ আজ হুমকি মুখে মানুষকে বিপদগ্রস্ত করার জন্য সে সর্বদা লিপ্ত থাকে\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veo899QJDKar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_words(count_dict, text):\n",
        "    for sentence in text:\n",
        "        for word in sentence.split():\n",
        "            if word not in count_dict:\n",
        "                count_dict[word] = 1\n",
        "            else:\n",
        "                count_dict[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zdvxJZbDKax",
        "colab_type": "code",
        "outputId": "30a083a9-2251-4100-ed39-b9da04e6206a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "word_counts = {}\n",
        "count_words(word_counts, clean_summaries)\n",
        "count_words(word_counts, clean_texts)            \n",
        "print(\"Size of Vocabulary:\", len(word_counts))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of Vocabulary: 5134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9DbQnZwDKa3",
        "colab_type": "code",
        "outputId": "f67a8943-ee28-4ae8-9c32-3ea1394a453b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "word_counts[\"দেয়া\"]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGANhEEUDKa_",
        "colab_type": "code",
        "outputId": "80f02441-1754-405d-af88-df4c466a22fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import numpy as np\n",
        "embeddings_index = {}\n",
        "with open('gdrive/My Drive/Colab Notebooks/bn_w2v_model.text', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split(' ')\n",
        "        word = values[0]\n",
        "        embedding = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = embedding\n",
        "\n",
        "print('Word embeddings:', len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word embeddings: 497405\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5pZaRRVDKbI",
        "colab_type": "code",
        "outputId": "f47aac70-d1b4-41f0-d5dc-2694ee8b4914",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "embeddings_index[\"জাতীয়\"]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2.05410e-02,  6.26330e-02, -5.53490e-02, -2.54590e-02,\n",
              "        6.18490e-02,  2.27295e-01, -1.29583e-01,  4.57040e-02,\n",
              "        3.60720e-02,  8.10060e-02, -1.31711e-01, -1.11894e-01,\n",
              "        1.43287e-01, -2.13770e-02, -3.22964e-01,  2.46480e-02,\n",
              "        8.61500e-02, -5.79400e-02,  8.86090e-02,  8.51600e-02,\n",
              "        6.89690e-02, -1.11190e-02,  3.45341e-01, -8.32900e-02,\n",
              "       -2.80570e-02, -9.46200e-03,  3.60380e-02,  2.17625e-01,\n",
              "       -6.20290e-02, -1.26047e-01, -1.38668e-01,  2.20440e-02,\n",
              "        8.01260e-02,  1.38463e-01, -1.07327e-01, -1.64391e-01,\n",
              "        8.87740e-02, -5.59450e-02,  8.10050e-02,  3.00634e-01,\n",
              "        2.22209e-01,  7.90780e-02,  1.71066e-01, -1.06080e-02,\n",
              "        6.86540e-02,  2.15358e-01,  8.28820e-02, -5.63270e-02,\n",
              "       -2.28050e-02, -8.81600e-02, -2.08755e-01, -4.14620e-02,\n",
              "       -3.19320e-02,  7.80410e-02,  3.07990e-02,  7.03770e-02,\n",
              "       -6.82880e-02,  3.51410e-02, -6.10490e-02, -2.59885e-01,\n",
              "       -2.71876e-01,  1.55580e-02,  9.79880e-02,  1.09827e-01,\n",
              "        3.10850e-02, -4.21980e-02, -6.00570e-02,  1.46701e-01,\n",
              "       -1.91746e-01,  1.75303e-01, -7.56020e-02,  2.73142e-01,\n",
              "        5.31250e-02,  1.74924e-01, -1.20288e-01,  1.17599e-01,\n",
              "        3.35180e-02,  4.74650e-02,  1.23473e-01,  4.00830e-02,\n",
              "        1.07855e-01, -3.72010e-02, -1.30610e-01, -5.69700e-02,\n",
              "        1.78766e-01, -1.18834e-01, -1.11804e-01, -1.22200e-02,\n",
              "        1.42180e-01,  9.35900e-03,  3.61640e-02, -3.87350e-02,\n",
              "       -2.94666e-01, -1.74020e-01,  1.32890e-02,  7.07660e-02,\n",
              "        2.02473e-01, -1.25324e-01,  1.27112e-01, -1.18834e-01,\n",
              "        3.12493e-01, -5.04280e-02,  3.71170e-02, -1.03934e-01,\n",
              "       -3.13890e-02,  8.45130e-02,  2.45800e-03,  2.56844e-01,\n",
              "       -2.09111e-01, -1.48620e-01, -1.66505e-01,  1.35450e-02,\n",
              "       -1.42589e-01,  2.80583e-01,  1.19107e-01,  3.24420e-02,\n",
              "        1.06153e-01, -3.06310e-02, -4.17870e-02,  9.33420e-02,\n",
              "       -5.14190e-02,  2.50475e-01, -3.07500e-01, -2.67630e-02,\n",
              "       -9.77940e-02,  8.74770e-02,  5.93560e-02, -4.14846e-01,\n",
              "       -3.77010e-02, -1.94183e-01, -8.06550e-02, -1.49484e-01,\n",
              "       -5.99760e-02, -1.98677e-01,  1.08279e-01,  1.26575e-01,\n",
              "       -1.48403e-01, -1.92593e-01, -2.64770e-02, -5.89970e-02,\n",
              "       -2.20963e-01, -1.79310e-01, -1.73251e-01, -1.62942e-01,\n",
              "       -1.58848e-01, -1.25680e-02, -6.42490e-02, -1.65488e-01,\n",
              "       -1.30516e-01,  9.00450e-02, -2.54740e-02,  2.27880e-02,\n",
              "        1.48193e-01, -3.18616e-01, -7.00410e-02,  1.59200e-02,\n",
              "        3.62110e-02, -1.13170e-02, -1.50700e-01, -2.17230e-02,\n",
              "       -2.09670e-02, -2.18406e-01,  1.47709e-01,  6.66850e-02,\n",
              "        9.05600e-03,  3.04180e-02,  6.28840e-02, -1.14266e-01,\n",
              "        9.80370e-02, -7.74700e-03,  4.65810e-02, -1.12580e-02,\n",
              "       -7.71270e-02,  1.31204e-01,  2.09780e-02,  6.57070e-02,\n",
              "       -1.55410e-01, -4.58900e-03, -1.37301e-01, -9.89130e-02,\n",
              "       -2.30561e-01, -5.26000e-04,  1.77451e-01, -3.97680e-02,\n",
              "       -1.58343e-01, -1.09670e-02,  1.09941e-01, -1.70477e-01,\n",
              "        1.33490e-02, -1.32206e-01, -4.55600e-03,  6.46380e-02,\n",
              "       -1.74557e-01,  3.79300e-02, -1.20047e-01,  3.25857e-01,\n",
              "        3.13838e-01,  5.52500e-02, -4.77460e-02, -2.39500e-03,\n",
              "        3.66990e-02,  2.05000e-03,  2.66273e-01,  2.63368e-01,\n",
              "        1.15523e-01,  2.41999e-01, -3.89350e-02, -1.38928e-01,\n",
              "       -6.94030e-02, -8.07760e-02,  1.88154e-01,  6.71350e-02,\n",
              "       -4.08380e-02, -1.09416e-01,  1.71999e-01, -2.27683e-01,\n",
              "        1.04273e-01, -6.59860e-02, -9.73010e-02,  1.59745e-01,\n",
              "       -1.78655e-01,  2.51110e-02, -1.27140e-01,  5.34650e-02,\n",
              "       -7.76700e-02, -1.70201e-01,  2.76461e-01, -1.08624e-01,\n",
              "       -6.79660e-02, -7.61260e-02,  1.14872e-01,  1.47351e-01,\n",
              "       -9.48110e-02, -1.96020e-02,  8.20000e-05,  1.15261e-01,\n",
              "       -6.66160e-02, -5.57430e-02, -2.36985e-01, -1.59865e-01,\n",
              "       -5.38230e-02,  2.28222e-01,  1.82748e-01, -1.23190e-01,\n",
              "       -1.63350e-02,  2.61400e-03, -1.60659e-01, -6.50890e-02,\n",
              "       -9.16620e-02,  1.29832e-01, -1.02635e-01, -4.66910e-02,\n",
              "        8.80010e-02,  3.07060e-02, -2.85478e-01,  3.27850e-02,\n",
              "       -8.60080e-02, -1.93383e-01,  9.75630e-02,  1.69290e-01,\n",
              "       -4.00690e-02,  3.21337e-01, -1.98800e-02, -3.09000e-04,\n",
              "        2.05522e-01, -1.80800e-01, -1.43985e-01,  2.83691e-01,\n",
              "       -1.98185e-01,  1.04180e-02,  3.79380e-02,  1.92834e-01,\n",
              "        9.73550e-02, -3.01850e-02,  1.31395e-01,  4.16600e-03,\n",
              "       -1.73148e-01, -1.60143e-01,  8.19950e-02,  4.10143e-01,\n",
              "       -1.05157e-01, -6.05810e-02,  1.23674e-01, -1.16269e-01,\n",
              "        1.81575e-01, -6.36940e-02, -1.18455e-01,  1.38480e-01,\n",
              "        3.29950e-02, -1.52042e-01,  2.24914e-01, -1.00939e-01,\n",
              "       -7.34060e-02,  6.40110e-02, -8.76890e-02,  1.23521e-01,\n",
              "        2.53300e-02,  5.27080e-02, -1.71612e-01, -1.17880e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dzvXzWuDKbQ",
        "colab_type": "code",
        "outputId": "77267499-c8ae-424a-de48-63add3bcd9f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "missing_words = 0\n",
        "threshold = 5\n",
        "\n",
        "for word, count in word_counts.items():\n",
        "    if count > threshold:\n",
        "        if word not in embeddings_index:\n",
        "            missing_words += 1\n",
        "            \n",
        "missing_ratio = round(missing_words/len(word_counts),2)*100            \n",
        "print(\"Number of words missing from cc-bn:\", missing_words)\n",
        "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words missing from cc-bn: 0\n",
            "Percent of words that are missing from vocabulary: 0.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnFRD3BJDKbY",
        "colab_type": "code",
        "outputId": "aae4610a-49a2-4be0-9be1-fae47a0890cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "vocab_to_int = {} \n",
        "\n",
        "value = 0\n",
        "for word, count in word_counts.items():\n",
        "    if count >= threshold or word in embeddings_index:\n",
        "        vocab_to_int[word] = value\n",
        "        value += 1\n",
        "\n",
        "# Special tokens that will be added to our vocab\n",
        "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
        "\n",
        "# Add codes to vocab\n",
        "for code in codes:\n",
        "    vocab_to_int[code] = len(vocab_to_int)\n",
        "\n",
        "# Dictionary to convert integers to words\n",
        "int_to_vocab = {}\n",
        "for word, value in vocab_to_int.items():\n",
        "    int_to_vocab[value] = word\n",
        "\n",
        "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
        "\n",
        "print(\"Total number of unique words:\", len(word_counts))\n",
        "print(\"Number of words we will use:\", len(vocab_to_int))\n",
        "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of unique words: 5134\n",
            "Number of words we will use: 4755\n",
            "Percent of words we will use: 92.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOxzlESxDKbg",
        "colab_type": "code",
        "outputId": "fc53fc67-c128-4d53-b7cb-648fca9461ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "embedding_dim = 300\n",
        "nb_words = len(vocab_to_int)\n",
        "\n",
        "# Create matrix with default values of zero\n",
        "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
        "for word, i in vocab_to_int.items():\n",
        "    if word in embeddings_index:\n",
        "        word_embedding_matrix[i] = embeddings_index[word]\n",
        "    else:\n",
        "        # If word not in CN, create a random embedding for it\n",
        "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
        "        embeddings_index[word] = new_embedding\n",
        "        word_embedding_matrix[i] = new_embedding\n",
        "\n",
        "# Check if value matches len(vocab_to_int)\n",
        "print(len(word_embedding_matrix))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4755\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUnXOs1HDKbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
        "    '''Convert words in text to an integer.\n",
        "       If word is not in vocab_to_int, use UNK's integer.\n",
        "       Total the number of words and UNKs.\n",
        "       Add EOS token to the end of texts'''\n",
        "    ints = []\n",
        "    for sentence in text:\n",
        "        sentence_ints = []\n",
        "        for word in sentence.split():\n",
        "            word_count += 1\n",
        "            if word in vocab_to_int:\n",
        "                sentence_ints.append(vocab_to_int[word])\n",
        "            else:\n",
        "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
        "                unk_count += 1\n",
        "        if eos:\n",
        "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
        "        ints.append(sentence_ints)\n",
        "    return ints, word_count, unk_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rvcp86sBDKby",
        "colab_type": "code",
        "outputId": "6a05baff-a7b7-4bd3-d029-f104a553af83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "word_count = 0\n",
        "unk_count = 0\n",
        "\n",
        "int_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)\n",
        "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\n",
        "\n",
        "unk_percent = round(unk_count/word_count,4)*100\n",
        "\n",
        "print(\"Total number of words in headlines:\", word_count)\n",
        "print(\"Total number of UNKs in headlines:\", unk_count)\n",
        "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of words in headlines: 15456\n",
            "Total number of UNKs in headlines: 445\n",
            "Percent of words that are UNK: 2.88%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9c6G41QDKb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_lengths(text):\n",
        "    '''Create a data frame of the sentence lengths from a text'''\n",
        "    lengths = []\n",
        "    for sentence in text:\n",
        "        lengths.append(len(sentence))\n",
        "    return pd.DataFrame(lengths, columns=['counts'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmbUlqtHDKb8",
        "colab_type": "code",
        "outputId": "d0fafe70-eb22-4585-d8ec-e7873bc8da78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        }
      },
      "source": [
        "lengths_summaries = create_lengths(int_summaries)\n",
        "lengths_texts = create_lengths(int_texts)\n",
        "\n",
        "print(\"Summaries:\")\n",
        "print(lengths_summaries.describe())\n",
        "print()\n",
        "print(\"Texts:\")\n",
        "print(lengths_texts.describe())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Summaries:\n",
            "           counts\n",
            "count  200.000000\n",
            "mean     6.245000\n",
            "std      2.513276\n",
            "min      2.000000\n",
            "25%      4.000000\n",
            "50%      6.000000\n",
            "75%      8.000000\n",
            "max     15.000000\n",
            "\n",
            "Texts:\n",
            "           counts\n",
            "count  200.000000\n",
            "mean    72.035000\n",
            "std     54.216148\n",
            "min     12.000000\n",
            "25%     35.750000\n",
            "50%     50.500000\n",
            "75%     87.500000\n",
            "max    302.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkqYQx7eDKcD",
        "colab_type": "code",
        "outputId": "2b07fa00-08bf-45a0-f7bb-cb8a6ee029a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "\n",
        "print(np.percentile(lengths_texts.counts, 90))\n",
        "print(np.percentile(lengths_texts.counts, 95))\n",
        "print(np.percentile(lengths_texts.counts, 99))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "144.89999999999995\n",
            "187.19999999999993\n",
            "274.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnziffEVDKcH",
        "colab_type": "code",
        "outputId": "51b2d2f5-d981-4601-ab9d-a8c7916e1227",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "print(np.percentile(lengths_summaries.counts, 90))\n",
        "print(np.percentile(lengths_summaries.counts, 95))\n",
        "print(np.percentile(lengths_summaries.counts, 99))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.0\n",
            "11.0\n",
            "14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s7rHxfjDKcO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unk_counter(sentence):\n",
        "    '''Counts the number of time UNK appears in a sentence.'''\n",
        "    unk_count = 0\n",
        "    for word in sentence:\n",
        "        if word == vocab_to_int[\"<UNK>\"]:\n",
        "            unk_count += 1\n",
        "    return unk_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQ2v8CyFDKcT",
        "colab_type": "code",
        "outputId": "01c46e64-0cc0-4f4a-fc2f-7e6778cf3d0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "sorted_summaries = []\n",
        "sorted_texts = []\n",
        "max_text_length = 83\n",
        "max_summary_length =13\n",
        "min_length = 2\n",
        "unk_text_limit = 1\n",
        "unk_summary_limit = 0\n",
        "\n",
        "for length in range(min(lengths_texts.counts), max_text_length): \n",
        "    for count, words in enumerate(int_summaries):\n",
        "        if (len(int_summaries[count]) >= min_length and\n",
        "            len(int_summaries[count]) <= max_summary_length and\n",
        "            len(int_texts[count]) >= min_length and\n",
        "            unk_counter(int_summaries[count]) <= unk_summary_limit and\n",
        "            unk_counter(int_texts[count]) <= unk_text_limit and\n",
        "            length == len(int_texts[count])\n",
        "           ):\n",
        "            sorted_summaries.append(int_summaries[count])\n",
        "            sorted_texts.append(int_texts[count])\n",
        "        \n",
        "# Compare lengths to ensure they match\n",
        "print(len(sorted_summaries))\n",
        "print(len(sorted_texts))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "78\n",
            "78\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrKKxSBMDKcc",
        "colab_type": "code",
        "outputId": "9c108eba-e724-4a55-b1bb-4d8637117ef8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.python.layers.core import Dense\n",
        "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
        "print('TensorFlow Version: {}'.format(tf.__version__))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "TensorFlow Version: 1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPLQAM2NDKcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_inputs():\n",
        "    '''Create palceholders for inputs to the model'''\n",
        "    \n",
        "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
        "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
        "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
        "\n",
        "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uhoy_lJoDKcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
        "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
        "    \n",
        "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
        "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
        "\n",
        "    return dec_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFpfds-kDKcx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
        "    '''Create the encoding layer'''\n",
        "    \n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
        "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
        "                                                                    cell_bw, \n",
        "                                                                    rnn_inputs,\n",
        "                                                                    sequence_length,\n",
        "                                                                    dtype=tf.float32)\n",
        "    # Join outputs since we are using a bidirectional RNN\n",
        "    enc_output = tf.concat(enc_output,2)\n",
        "    \n",
        "    return enc_output, enc_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYjesQuHDKc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
        "                            vocab_size, max_summary_length):\n",
        "    '''Create the training logits'''\n",
        "    \n",
        "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
        "                                                        sequence_length=summary_length,\n",
        "                                                        time_major=False)\n",
        "\n",
        "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                       training_helper,\n",
        "                                                       initial_state,\n",
        "                                                       output_layer) \n",
        "\n",
        "    training_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                                           output_time_major=False,\n",
        "                                                           impute_finished=True,\n",
        "                                                           maximum_iterations=max_summary_length)\n",
        "    return training_decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZHMP7evDKc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
        "                             max_summary_length, batch_size):\n",
        "    '''Create the inference logits'''\n",
        "    \n",
        "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
        "    \n",
        "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
        "                                                                start_tokens,\n",
        "                                                                end_token)\n",
        "                \n",
        "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                        inference_helper,\n",
        "                                                        initial_state,\n",
        "                                                        output_layer)\n",
        "                \n",
        "    inference_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                                            output_time_major=False,\n",
        "                                                            impute_finished=True,\n",
        "                                                            maximum_iterations=max_summary_length)\n",
        "    \n",
        "    return inference_decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap_3StUCDKc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
        "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
        "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
        "    \n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
        "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
        "                                                     input_keep_prob = keep_prob)\n",
        "    \n",
        "    output_layer = Dense(vocab_size,\n",
        "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
        "    \n",
        "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
        "                                                  enc_output,\n",
        "                                                  text_length,\n",
        "                                                  normalize=False,\n",
        "                                                  name='BahdanauAttention')\n",
        "\n",
        "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n",
        "                                                          attn_mech,\n",
        "                                                          rnn_size)\n",
        "            \n",
        "    #initial_state = tf.contrib.seq2seq.AttentionWrapperState(enc_state[0],\n",
        "    #                                                                _zero_state_tensors(rnn_size, \n",
        "    #                                                                                    batch_size, \n",
        "    #                                                                                    tf.float32)) \n",
        "    initial_state = dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state[0])\n",
        "\n",
        "    with tf.variable_scope(\"decode\"):\n",
        "        training_decoder = training_decoding_layer(dec_embed_input, \n",
        "                                                  summary_length, \n",
        "                                                  dec_cell, \n",
        "                                                  initial_state,\n",
        "                                                  output_layer,\n",
        "                                                  vocab_size, \n",
        "                                                  max_summary_length)\n",
        "        \n",
        "        training_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                  output_time_major=False,\n",
        "                                  impute_finished=True,\n",
        "                                  maximum_iterations=max_summary_length)\n",
        "    with tf.variable_scope(\"decode\", reuse=True):\n",
        "        inference_decoder = inference_decoding_layer(embeddings,  \n",
        "                                                    vocab_to_int['<GO>'], \n",
        "                                                    vocab_to_int['<EOS>'],\n",
        "                                                    dec_cell, \n",
        "                                                    initial_state, \n",
        "                                                    output_layer,\n",
        "                                                    max_summary_length,\n",
        "                                                    batch_size)\n",
        "        \n",
        "        inference_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                  output_time_major=False,\n",
        "                                  impute_finished=True,\n",
        "                                  maximum_iterations=max_summary_length)\n",
        "\n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzCxx7mVDKdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
        "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
        "    '''Use the previous functions to create the training and inference logits'''\n",
        "    \n",
        "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
        "    embeddings = word_embedding_matrix\n",
        "    \n",
        "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
        "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
        "    \n",
        "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
        "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
        "    \n",
        "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
        "                                                        embeddings,\n",
        "                                                        enc_output,\n",
        "                                                        enc_state, \n",
        "                                                        vocab_size, \n",
        "                                                        text_length, \n",
        "                                                        summary_length, \n",
        "                                                        max_summary_length,\n",
        "                                                        rnn_size, \n",
        "                                                        vocab_to_int, \n",
        "                                                        keep_prob, \n",
        "                                                        batch_size,\n",
        "                                                        num_layers)\n",
        "    \n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhuXMMGGDKdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_sentence_batch(sentence_batch):\n",
        "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5uNNbXZDKdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(summaries, texts, batch_size):\n",
        "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
        "    for batch_i in range(0, len(texts)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
        "        texts_batch = texts[start_i:start_i + batch_size]\n",
        "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
        "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
        "        \n",
        "        # Need the lengths for the _lengths parameters\n",
        "        pad_summaries_lengths = []\n",
        "        for summary in pad_summaries_batch:\n",
        "            pad_summaries_lengths.append(len(summary))\n",
        "        \n",
        "        pad_texts_lengths = []\n",
        "        for text in pad_texts_batch:\n",
        "            pad_texts_lengths.append(len(text))\n",
        "        \n",
        "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zy2b5M95DKdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 30\n",
        "batch_size = 2\n",
        "rnn_size = 256\n",
        "num_layers = 3\n",
        "learning_rate = 0.001\n",
        "keep_probability = 0.70"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HACGJ9peDKdc",
        "colab_type": "code",
        "outputId": "81be1e6b-0992-428a-e5cd-bd51bfbd9e7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        }
      },
      "source": [
        "train_graph = tf.Graph()\n",
        "# Set the graph to default to ensure that it is ready for training\n",
        "with train_graph.as_default():\n",
        "    \n",
        "    # Load the model inputs    \n",
        "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
        "\n",
        "    # Create the training and inference logits\n",
        "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
        "                                                      targets, \n",
        "                                                      keep_prob,   \n",
        "                                                      text_length,\n",
        "                                                      summary_length,\n",
        "                                                      max_summary_length,\n",
        "                                                      len(vocab_to_int)+1,\n",
        "                                                      rnn_size, \n",
        "                                                      num_layers, \n",
        "                                                      vocab_to_int,\n",
        "                                                      batch_size)\n",
        "    \n",
        "    # Create tensors for the training logits and inference logits\n",
        "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
        "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
        "    \n",
        "    # Create the weights for sequence_loss\n",
        "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
        "\n",
        "    with tf.name_scope(\"optimization\"):\n",
        "        # Loss function\n",
        "        cost = tf.contrib.seq2seq.sequence_loss(\n",
        "            training_logits,\n",
        "            targets,\n",
        "            masks)\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "        # Gradient Clipping\n",
        "        gradients = optimizer.compute_gradients(cost)\n",
        "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
        "        train_op = optimizer.apply_gradients(capped_gradients)\n",
        "print(\"Graph is built.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-31-9390cb26606a>:7: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-31-9390cb26606a>:20: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Graph is built.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cmhj78EODKdh",
        "colab_type": "code",
        "outputId": "8a14c37c-f368-4a3e-a764-9acab7bab398",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "start = 12\n",
        "end = start + 83\n",
        "sorted_summaries_short = sorted_summaries[start:end]\n",
        "sorted_texts_short = sorted_texts[start:end]\n",
        "print(\"The shortest text length:\", len(sorted_texts_short[0]))\n",
        "print(\"The longest text length:\",len(sorted_texts_short[-1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shortest text length: 27\n",
            "The longest text length: 82\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jTWpc4gDKdo",
        "colab_type": "code",
        "outputId": "13ccd082-95e0-48a4-81f5-fe4201f79345",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train the Model\n",
        "learning_rate_decay = 0.90\n",
        "min_learning_rate = 0.001\n",
        "display_step = 2# Check training loss after every 20 batches\n",
        "stop_early = 0 \n",
        "stop = 3 #3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
        "per_epoch = 1 # Make 3 update checks per epoch\n",
        "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
        "\n",
        "update_loss = 0 \n",
        "batch_loss = 0\n",
        "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
        "\n",
        "  \n",
        "tf.reset_default_graph()\n",
        "checkpoint = \"./model1.ckpt\"  #300k sentence\n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # If we want to continue training a previous session\n",
        "    # loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    # loader.restore(sess, checkpoint)\n",
        "    #sess.run(tf.local_variables_initializer())\n",
        "\n",
        "    for epoch_i in range(1, epochs+1):\n",
        "        update_loss = 0\n",
        "        batch_loss = 0\n",
        "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
        "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
        "            start_time = time.time()\n",
        "            _, loss = sess.run(\n",
        "                [train_op, cost],\n",
        "                {input_data: texts_batch,\n",
        "                 targets: summaries_batch,\n",
        "                 lr: learning_rate,\n",
        "                 summary_length: summaries_lengths,\n",
        "                 text_length: texts_lengths,\n",
        "                 keep_prob: keep_probability})\n",
        "\n",
        "            batch_loss += loss\n",
        "            update_loss += loss\n",
        "            end_time = time.time()\n",
        "            batch_time = end_time - start_time\n",
        "\n",
        "            if batch_i % display_step == 0 and batch_i > 0:\n",
        "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
        "                      .format(epoch_i,\n",
        "                              epochs, \n",
        "                              batch_i, \n",
        "                              len(sorted_texts_short) // batch_size, \n",
        "                              batch_loss / display_step, \n",
        "                              batch_time*display_step))\n",
        "                batch_loss = 0\n",
        "                \n",
        "                #saver = tf.train.Saver() \n",
        "                #saver.save(sess, checkpoint)\n",
        "                \n",
        "            if batch_i % update_check == 0 and batch_i > 0:\n",
        "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
        "                summary_update_loss.append(update_loss)\n",
        "                \n",
        "              \n",
        "                  \n",
        "                # If the update loss is at a new minimum, save the model\n",
        "                if update_loss <= min(summary_update_loss):\n",
        "                    print('New Record!') \n",
        "                    stop_early = 0\n",
        "                    saver = tf.train.Saver() \n",
        "                    saver.save(sess, checkpoint)\n",
        "\n",
        "                else:\n",
        "                    print(\"No Improvement.\")\n",
        "                    stop_early += 1\n",
        "                    if stop_early == stop:\n",
        "                        break\n",
        "                update_loss = 0\n",
        "            \n",
        "                    \n",
        "        # Reduce learning rate, but not below its minimum value\n",
        "        learning_rate *= learning_rate_decay\n",
        "        if learning_rate < min_learning_rate:\n",
        "            learning_rate = min_learning_rate\n",
        "        \n",
        "        if stop_early == stop:\n",
        "            print(\"Stopping Training.\")\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch   1/30 Batch    2/33 - Loss: 12.581, Seconds: 0.26\n",
            "Epoch   1/30 Batch    4/33 - Loss:  8.098, Seconds: 0.22\n",
            "Epoch   1/30 Batch    6/33 - Loss:  7.862, Seconds: 0.26\n",
            "Epoch   1/30 Batch    8/33 - Loss:  7.948, Seconds: 0.19\n",
            "Epoch   1/30 Batch   10/33 - Loss:  7.451, Seconds: 0.21\n",
            "Epoch   1/30 Batch   12/33 - Loss:  8.632, Seconds: 0.22\n",
            "Epoch   1/30 Batch   14/33 - Loss:  7.209, Seconds: 0.27\n",
            "Epoch   1/30 Batch   16/33 - Loss:  7.782, Seconds: 0.35\n",
            "Epoch   1/30 Batch   18/33 - Loss:  7.935, Seconds: 0.35\n",
            "Epoch   1/30 Batch   20/33 - Loss:  7.899, Seconds: 0.26\n",
            "Epoch   1/30 Batch   22/33 - Loss:  8.112, Seconds: 0.28\n",
            "Epoch   1/30 Batch   24/33 - Loss:  7.417, Seconds: 0.28\n",
            "Epoch   1/30 Batch   26/33 - Loss:  7.065, Seconds: 0.34\n",
            "Epoch   1/30 Batch   28/33 - Loss:  6.043, Seconds: 0.42\n",
            "Epoch   1/30 Batch   30/33 - Loss:  8.221, Seconds: 0.40\n",
            "Epoch   1/30 Batch   32/33 - Loss:  6.282, Seconds: 0.59\n",
            "Average loss for this update: 7.909\n",
            "New Record!\n",
            "Epoch   2/30 Batch    2/33 - Loss:  9.104, Seconds: 0.19\n",
            "Epoch   2/30 Batch    4/33 - Loss:  5.387, Seconds: 0.20\n",
            "Epoch   2/30 Batch    6/33 - Loss:  5.635, Seconds: 0.20\n",
            "Epoch   2/30 Batch    8/33 - Loss:  5.309, Seconds: 0.20\n",
            "Epoch   2/30 Batch   10/33 - Loss:  5.111, Seconds: 0.30\n",
            "Epoch   2/30 Batch   12/33 - Loss:  5.986, Seconds: 0.24\n",
            "Epoch   2/30 Batch   14/33 - Loss:  5.094, Seconds: 0.30\n",
            "Epoch   2/30 Batch   16/33 - Loss:  5.816, Seconds: 0.32\n",
            "Epoch   2/30 Batch   18/33 - Loss:  5.842, Seconds: 0.27\n",
            "Epoch   2/30 Batch   20/33 - Loss:  5.670, Seconds: 0.33\n",
            "Epoch   2/30 Batch   22/33 - Loss:  6.306, Seconds: 0.36\n",
            "Epoch   2/30 Batch   24/33 - Loss:  5.701, Seconds: 0.40\n",
            "Epoch   2/30 Batch   26/33 - Loss:  5.534, Seconds: 0.41\n",
            "Epoch   2/30 Batch   28/33 - Loss:  5.000, Seconds: 0.49\n",
            "Epoch   2/30 Batch   30/33 - Loss:  6.397, Seconds: 0.44\n",
            "Epoch   2/30 Batch   32/33 - Loss:  5.130, Seconds: 0.48\n",
            "Average loss for this update: 5.814\n",
            "New Record!\n",
            "Epoch   3/30 Batch    2/33 - Loss:  6.891, Seconds: 0.26\n",
            "Epoch   3/30 Batch    4/33 - Loss:  4.632, Seconds: 0.17\n",
            "Epoch   3/30 Batch    6/33 - Loss:  4.872, Seconds: 0.20\n",
            "Epoch   3/30 Batch    8/33 - Loss:  4.761, Seconds: 0.25\n",
            "Epoch   3/30 Batch   10/33 - Loss:  4.431, Seconds: 0.29\n",
            "Epoch   3/30 Batch   12/33 - Loss:  5.074, Seconds: 0.27\n",
            "Epoch   3/30 Batch   14/33 - Loss:  4.414, Seconds: 0.28\n",
            "Epoch   3/30 Batch   16/33 - Loss:  5.008, Seconds: 0.32\n",
            "Epoch   3/30 Batch   18/33 - Loss:  5.018, Seconds: 0.35\n",
            "Epoch   3/30 Batch   20/33 - Loss:  4.760, Seconds: 0.35\n",
            "Epoch   3/30 Batch   22/33 - Loss:  5.821, Seconds: 0.34\n",
            "Epoch   3/30 Batch   24/33 - Loss:  4.994, Seconds: 0.40\n",
            "Epoch   3/30 Batch   26/33 - Loss:  4.857, Seconds: 0.35\n",
            "Epoch   3/30 Batch   28/33 - Loss:  4.261, Seconds: 0.39\n",
            "Epoch   3/30 Batch   30/33 - Loss:  4.944, Seconds: 0.41\n",
            "Epoch   3/30 Batch   32/33 - Loss:  4.728, Seconds: 0.58\n",
            "Average loss for this update: 4.967\n",
            "New Record!\n",
            "Epoch   4/30 Batch    2/33 - Loss:  5.337, Seconds: 0.24\n",
            "Epoch   4/30 Batch    4/33 - Loss:  4.343, Seconds: 0.24\n",
            "Epoch   4/30 Batch    6/33 - Loss:  4.819, Seconds: 0.28\n",
            "Epoch   4/30 Batch    8/33 - Loss:  4.627, Seconds: 0.28\n",
            "Epoch   4/30 Batch   10/33 - Loss:  4.091, Seconds: 0.23\n",
            "Epoch   4/30 Batch   12/33 - Loss:  4.856, Seconds: 0.28\n",
            "Epoch   4/30 Batch   14/33 - Loss:  4.091, Seconds: 0.24\n",
            "Epoch   4/30 Batch   16/33 - Loss:  4.504, Seconds: 0.25\n",
            "Epoch   4/30 Batch   18/33 - Loss:  4.406, Seconds: 0.27\n",
            "Epoch   4/30 Batch   20/33 - Loss:  4.241, Seconds: 0.31\n",
            "Epoch   4/30 Batch   22/33 - Loss:  4.176, Seconds: 0.28\n",
            "Epoch   4/30 Batch   24/33 - Loss:  4.662, Seconds: 0.31\n",
            "Epoch   4/30 Batch   26/33 - Loss:  4.624, Seconds: 0.40\n",
            "Epoch   4/30 Batch   28/33 - Loss:  3.797, Seconds: 0.37\n",
            "Epoch   4/30 Batch   30/33 - Loss:  3.800, Seconds: 0.39\n",
            "Epoch   4/30 Batch   32/33 - Loss:  4.307, Seconds: 0.60\n",
            "Average loss for this update: 4.418\n",
            "New Record!\n",
            "Epoch   5/30 Batch    2/33 - Loss:  6.202, Seconds: 0.19\n",
            "Epoch   5/30 Batch    4/33 - Loss:  3.654, Seconds: 0.16\n",
            "Epoch   5/30 Batch    6/33 - Loss:  4.039, Seconds: 0.20\n",
            "Epoch   5/30 Batch    8/33 - Loss:  4.192, Seconds: 0.20\n",
            "Epoch   5/30 Batch   10/33 - Loss:  3.666, Seconds: 0.27\n",
            "Epoch   5/30 Batch   12/33 - Loss:  5.150, Seconds: 0.22\n",
            "Epoch   5/30 Batch   14/33 - Loss:  4.027, Seconds: 0.23\n",
            "Epoch   5/30 Batch   16/33 - Loss:  4.557, Seconds: 0.25\n",
            "Epoch   5/30 Batch   18/33 - Loss:  4.518, Seconds: 0.32\n",
            "Epoch   5/30 Batch   20/33 - Loss:  4.698, Seconds: 0.27\n",
            "Epoch   5/30 Batch   22/33 - Loss:  5.238, Seconds: 0.32\n",
            "Epoch   5/30 Batch   24/33 - Loss:  4.163, Seconds: 0.35\n",
            "Epoch   5/30 Batch   26/33 - Loss:  4.091, Seconds: 0.33\n",
            "Epoch   5/30 Batch   28/33 - Loss:  3.488, Seconds: 0.38\n",
            "Epoch   5/30 Batch   30/33 - Loss:  3.883, Seconds: 0.41\n",
            "Epoch   5/30 Batch   32/33 - Loss:  3.420, Seconds: 0.47\n",
            "Average loss for this update: 4.312\n",
            "New Record!\n",
            "Epoch   6/30 Batch    2/33 - Loss:  6.396, Seconds: 0.24\n",
            "Epoch   6/30 Batch    4/33 - Loss:  3.771, Seconds: 0.24\n",
            "Epoch   6/30 Batch    6/33 - Loss:  4.404, Seconds: 0.27\n",
            "Epoch   6/30 Batch    8/33 - Loss:  4.651, Seconds: 0.30\n",
            "Epoch   6/30 Batch   10/33 - Loss:  3.674, Seconds: 0.29\n",
            "Epoch   6/30 Batch   12/33 - Loss:  3.673, Seconds: 0.27\n",
            "Epoch   6/30 Batch   14/33 - Loss:  3.263, Seconds: 0.28\n",
            "Epoch   6/30 Batch   16/33 - Loss:  3.138, Seconds: 0.33\n",
            "Epoch   6/30 Batch   18/33 - Loss:  3.315, Seconds: 0.34\n",
            "Epoch   6/30 Batch   20/33 - Loss:  2.944, Seconds: 0.30\n",
            "Epoch   6/30 Batch   22/33 - Loss:  4.264, Seconds: 0.31\n",
            "Epoch   6/30 Batch   24/33 - Loss:  3.297, Seconds: 0.31\n",
            "Epoch   6/30 Batch   26/33 - Loss:  3.637, Seconds: 0.39\n",
            "Epoch   6/30 Batch   28/33 - Loss:  3.616, Seconds: 0.44\n",
            "Epoch   6/30 Batch   30/33 - Loss:  4.238, Seconds: 0.50\n",
            "Epoch   6/30 Batch   32/33 - Loss:  3.536, Seconds: 0.61\n",
            "Average loss for this update: 3.864\n",
            "New Record!\n",
            "Epoch   7/30 Batch    2/33 - Loss:  3.752, Seconds: 0.23\n",
            "Epoch   7/30 Batch    4/33 - Loss:  2.289, Seconds: 0.22\n",
            "Epoch   7/30 Batch    6/33 - Loss:  3.877, Seconds: 0.20\n",
            "Epoch   7/30 Batch    8/33 - Loss:  3.287, Seconds: 0.22\n",
            "Epoch   7/30 Batch   10/33 - Loss:  3.097, Seconds: 0.31\n",
            "Epoch   7/30 Batch   12/33 - Loss:  2.651, Seconds: 0.25\n",
            "Epoch   7/30 Batch   14/33 - Loss:  2.469, Seconds: 0.23\n",
            "Epoch   7/30 Batch   16/33 - Loss:  2.298, Seconds: 0.28\n",
            "Epoch   7/30 Batch   18/33 - Loss:  2.465, Seconds: 0.35\n",
            "Epoch   7/30 Batch   20/33 - Loss:  2.003, Seconds: 0.26\n",
            "Epoch   7/30 Batch   22/33 - Loss:  3.145, Seconds: 0.28\n",
            "Epoch   7/30 Batch   24/33 - Loss:  2.317, Seconds: 0.30\n",
            "Epoch   7/30 Batch   26/33 - Loss:  3.978, Seconds: 0.37\n",
            "Epoch   7/30 Batch   28/33 - Loss:  4.130, Seconds: 0.43\n",
            "Epoch   7/30 Batch   30/33 - Loss:  4.414, Seconds: 0.46\n",
            "Epoch   7/30 Batch   32/33 - Loss:  3.791, Seconds: 0.48\n",
            "Average loss for this update: 3.123\n",
            "New Record!\n",
            "Epoch   8/30 Batch    2/33 - Loss:  2.941, Seconds: 0.19\n",
            "Epoch   8/30 Batch    4/33 - Loss:  1.352, Seconds: 0.18\n",
            "Epoch   8/30 Batch    6/33 - Loss:  3.245, Seconds: 0.20\n",
            "Epoch   8/30 Batch    8/33 - Loss:  3.619, Seconds: 0.24\n",
            "Epoch   8/30 Batch   10/33 - Loss:  3.447, Seconds: 0.24\n",
            "Epoch   8/30 Batch   12/33 - Loss:  2.277, Seconds: 0.23\n",
            "Epoch   8/30 Batch   14/33 - Loss:  1.961, Seconds: 0.24\n",
            "Epoch   8/30 Batch   16/33 - Loss:  2.727, Seconds: 0.25\n",
            "Epoch   8/30 Batch   18/33 - Loss:  2.881, Seconds: 0.28\n",
            "Epoch   8/30 Batch   20/33 - Loss:  1.386, Seconds: 0.27\n",
            "Epoch   8/30 Batch   22/33 - Loss:  2.737, Seconds: 0.28\n",
            "Epoch   8/30 Batch   24/33 - Loss:  2.637, Seconds: 0.39\n",
            "Epoch   8/30 Batch   26/33 - Loss:  2.546, Seconds: 0.33\n",
            "Epoch   8/30 Batch   28/33 - Loss:  3.110, Seconds: 0.38\n",
            "Epoch   8/30 Batch   30/33 - Loss:  2.582, Seconds: 0.50\n",
            "Epoch   8/30 Batch   32/33 - Loss:  3.506, Seconds: 0.61\n",
            "Average loss for this update: 2.685\n",
            "New Record!\n",
            "Epoch   9/30 Batch    2/33 - Loss:  3.429, Seconds: 0.19\n",
            "Epoch   9/30 Batch    4/33 - Loss:  2.079, Seconds: 0.17\n",
            "Epoch   9/30 Batch    6/33 - Loss:  3.015, Seconds: 0.25\n",
            "Epoch   9/30 Batch    8/33 - Loss:  3.372, Seconds: 0.27\n",
            "Epoch   9/30 Batch   10/33 - Loss:  2.707, Seconds: 0.30\n",
            "Epoch   9/30 Batch   12/33 - Loss:  1.619, Seconds: 0.28\n",
            "Epoch   9/30 Batch   14/33 - Loss:  2.483, Seconds: 0.30\n",
            "Epoch   9/30 Batch   16/33 - Loss:  2.575, Seconds: 0.28\n",
            "Epoch   9/30 Batch   18/33 - Loss:  3.181, Seconds: 0.34\n",
            "Epoch   9/30 Batch   20/33 - Loss:  2.524, Seconds: 0.33\n",
            "Epoch   9/30 Batch   22/33 - Loss:  2.471, Seconds: 0.33\n",
            "Epoch   9/30 Batch   24/33 - Loss:  1.614, Seconds: 0.34\n",
            "Epoch   9/30 Batch   26/33 - Loss:  2.841, Seconds: 0.41\n",
            "Epoch   9/30 Batch   28/33 - Loss:  3.277, Seconds: 0.52\n",
            "Epoch   9/30 Batch   30/33 - Loss:  3.022, Seconds: 0.39\n",
            "Epoch   9/30 Batch   32/33 - Loss:  1.984, Seconds: 0.51\n",
            "Average loss for this update: 2.637\n",
            "New Record!\n",
            "Epoch  10/30 Batch    2/33 - Loss:  2.731, Seconds: 0.26\n",
            "Epoch  10/30 Batch    4/33 - Loss:  2.520, Seconds: 0.19\n",
            "Epoch  10/30 Batch    6/33 - Loss:  2.567, Seconds: 0.20\n",
            "Epoch  10/30 Batch    8/33 - Loss:  1.602, Seconds: 0.26\n",
            "Epoch  10/30 Batch   10/33 - Loss:  2.327, Seconds: 0.23\n",
            "Epoch  10/30 Batch   12/33 - Loss:  2.050, Seconds: 0.22\n",
            "Epoch  10/30 Batch   14/33 - Loss:  1.359, Seconds: 0.29\n",
            "Epoch  10/30 Batch   16/33 - Loss:  1.405, Seconds: 0.33\n",
            "Epoch  10/30 Batch   18/33 - Loss:  1.903, Seconds: 0.35\n",
            "Epoch  10/30 Batch   20/33 - Loss:  1.062, Seconds: 0.28\n",
            "Epoch  10/30 Batch   22/33 - Loss:  3.251, Seconds: 0.27\n",
            "Epoch  10/30 Batch   24/33 - Loss:  2.326, Seconds: 0.37\n",
            "Epoch  10/30 Batch   26/33 - Loss:  2.970, Seconds: 0.43\n",
            "Epoch  10/30 Batch   28/33 - Loss:  2.993, Seconds: 0.48\n",
            "Epoch  10/30 Batch   30/33 - Loss:  2.352, Seconds: 0.42\n",
            "Epoch  10/30 Batch   32/33 - Loss:  2.448, Seconds: 0.57\n",
            "Average loss for this update: 2.242\n",
            "New Record!\n",
            "Epoch  11/30 Batch    2/33 - Loss:  2.775, Seconds: 0.19\n",
            "Epoch  11/30 Batch    4/33 - Loss:  1.598, Seconds: 0.16\n",
            "Epoch  11/30 Batch    6/33 - Loss:  2.387, Seconds: 0.21\n",
            "Epoch  11/30 Batch    8/33 - Loss:  1.356, Seconds: 0.19\n",
            "Epoch  11/30 Batch   10/33 - Loss:  1.739, Seconds: 0.23\n",
            "Epoch  11/30 Batch   12/33 - Loss:  1.024, Seconds: 0.23\n",
            "Epoch  11/30 Batch   14/33 - Loss:  1.368, Seconds: 0.27\n",
            "Epoch  11/30 Batch   16/33 - Loss:  2.085, Seconds: 0.26\n",
            "Epoch  11/30 Batch   18/33 - Loss:  2.150, Seconds: 0.28\n",
            "Epoch  11/30 Batch   20/33 - Loss:  0.823, Seconds: 0.26\n",
            "Epoch  11/30 Batch   22/33 - Loss:  2.142, Seconds: 0.31\n",
            "Epoch  11/30 Batch   24/33 - Loss:  0.991, Seconds: 0.37\n",
            "Epoch  11/30 Batch   26/33 - Loss:  1.839, Seconds: 0.32\n",
            "Epoch  11/30 Batch   28/33 - Loss:  2.250, Seconds: 0.56\n",
            "Epoch  11/30 Batch   30/33 - Loss:  1.664, Seconds: 0.48\n",
            "Epoch  11/30 Batch   32/33 - Loss:  2.044, Seconds: 0.61\n",
            "Average loss for this update: 1.765\n",
            "New Record!\n",
            "Epoch  12/30 Batch    2/33 - Loss:  1.719, Seconds: 0.19\n",
            "Epoch  12/30 Batch    4/33 - Loss:  0.964, Seconds: 0.17\n",
            "Epoch  12/30 Batch    6/33 - Loss:  1.349, Seconds: 0.20\n",
            "Epoch  12/30 Batch    8/33 - Loss:  1.195, Seconds: 0.20\n",
            "Epoch  12/30 Batch   10/33 - Loss:  1.517, Seconds: 0.29\n",
            "Epoch  12/30 Batch   12/33 - Loss:  0.522, Seconds: 0.28\n",
            "Epoch  12/30 Batch   14/33 - Loss:  0.922, Seconds: 0.29\n",
            "Epoch  12/30 Batch   16/33 - Loss:  2.066, Seconds: 0.29\n",
            "Epoch  12/30 Batch   18/33 - Loss:  1.359, Seconds: 0.33\n",
            "Epoch  12/30 Batch   20/33 - Loss:  0.302, Seconds: 0.29\n",
            "Epoch  12/30 Batch   22/33 - Loss:  1.555, Seconds: 0.28\n",
            "Epoch  12/30 Batch   24/33 - Loss:  0.937, Seconds: 0.32\n",
            "Epoch  12/30 Batch   26/33 - Loss:  1.421, Seconds: 0.43\n",
            "Epoch  12/30 Batch   28/33 - Loss:  1.521, Seconds: 0.49\n",
            "Epoch  12/30 Batch   30/33 - Loss:  0.932, Seconds: 0.50\n",
            "Epoch  12/30 Batch   32/33 - Loss:  1.649, Seconds: 0.48\n",
            "Average loss for this update: 1.246\n",
            "New Record!\n",
            "Epoch  13/30 Batch    2/33 - Loss:  1.976, Seconds: 0.23\n",
            "Epoch  13/30 Batch    4/33 - Loss:  0.785, Seconds: 0.17\n",
            "Epoch  13/30 Batch    6/33 - Loss:  0.846, Seconds: 0.25\n",
            "Epoch  13/30 Batch    8/33 - Loss:  0.771, Seconds: 0.26\n",
            "Epoch  13/30 Batch   10/33 - Loss:  1.191, Seconds: 0.24\n",
            "Epoch  13/30 Batch   12/33 - Loss:  0.369, Seconds: 0.24\n",
            "Epoch  13/30 Batch   14/33 - Loss:  1.138, Seconds: 0.25\n",
            "Epoch  13/30 Batch   16/33 - Loss:  0.741, Seconds: 0.32\n",
            "Epoch  13/30 Batch   18/33 - Loss:  0.793, Seconds: 0.31\n",
            "Epoch  13/30 Batch   20/33 - Loss:  0.279, Seconds: 0.27\n",
            "Epoch  13/30 Batch   22/33 - Loss:  0.643, Seconds: 0.41\n",
            "Epoch  13/30 Batch   24/33 - Loss:  0.475, Seconds: 0.32\n",
            "Epoch  13/30 Batch   26/33 - Loss:  1.162, Seconds: 0.34\n",
            "Epoch  13/30 Batch   28/33 - Loss:  1.328, Seconds: 0.48\n",
            "Epoch  13/30 Batch   30/33 - Loss:  0.680, Seconds: 0.39\n",
            "Epoch  13/30 Batch   32/33 - Loss:  1.049, Seconds: 0.57\n",
            "Average loss for this update: 0.889\n",
            "New Record!\n",
            "Epoch  14/30 Batch    2/33 - Loss:  1.165, Seconds: 0.24\n",
            "Epoch  14/30 Batch    4/33 - Loss:  0.456, Seconds: 0.16\n",
            "Epoch  14/30 Batch    6/33 - Loss:  0.549, Seconds: 0.23\n",
            "Epoch  14/30 Batch    8/33 - Loss:  0.315, Seconds: 0.20\n",
            "Epoch  14/30 Batch   10/33 - Loss:  0.999, Seconds: 0.29\n",
            "Epoch  14/30 Batch   12/33 - Loss:  0.346, Seconds: 0.22\n",
            "Epoch  14/30 Batch   14/33 - Loss:  1.094, Seconds: 0.23\n",
            "Epoch  14/30 Batch   16/33 - Loss:  0.728, Seconds: 0.27\n",
            "Epoch  14/30 Batch   18/33 - Loss:  0.759, Seconds: 0.30\n",
            "Epoch  14/30 Batch   20/33 - Loss:  0.187, Seconds: 0.29\n",
            "Epoch  14/30 Batch   22/33 - Loss:  0.570, Seconds: 0.30\n",
            "Epoch  14/30 Batch   24/33 - Loss:  0.530, Seconds: 0.33\n",
            "Epoch  14/30 Batch   26/33 - Loss:  0.693, Seconds: 0.36\n",
            "Epoch  14/30 Batch   28/33 - Loss:  0.867, Seconds: 0.41\n",
            "Epoch  14/30 Batch   30/33 - Loss:  0.486, Seconds: 0.42\n",
            "Epoch  14/30 Batch   32/33 - Loss:  1.018, Seconds: 0.61\n",
            "Average loss for this update: 0.673\n",
            "New Record!\n",
            "Epoch  15/30 Batch    2/33 - Loss:  1.107, Seconds: 0.20\n",
            "Epoch  15/30 Batch    4/33 - Loss:  0.155, Seconds: 0.17\n",
            "Epoch  15/30 Batch    6/33 - Loss:  0.551, Seconds: 0.24\n",
            "Epoch  15/30 Batch    8/33 - Loss:  0.330, Seconds: 0.20\n",
            "Epoch  15/30 Batch   10/33 - Loss:  0.827, Seconds: 0.24\n",
            "Epoch  15/30 Batch   12/33 - Loss:  0.181, Seconds: 0.22\n",
            "Epoch  15/30 Batch   14/33 - Loss:  0.725, Seconds: 0.30\n",
            "Epoch  15/30 Batch   16/33 - Loss:  0.553, Seconds: 0.34\n",
            "Epoch  15/30 Batch   18/33 - Loss:  0.709, Seconds: 0.30\n",
            "Epoch  15/30 Batch   20/33 - Loss:  0.254, Seconds: 0.32\n",
            "Epoch  15/30 Batch   22/33 - Loss:  0.432, Seconds: 0.34\n",
            "Epoch  15/30 Batch   24/33 - Loss:  0.524, Seconds: 0.41\n",
            "Epoch  15/30 Batch   26/33 - Loss:  0.629, Seconds: 0.35\n",
            "Epoch  15/30 Batch   28/33 - Loss:  0.778, Seconds: 0.49\n",
            "Epoch  15/30 Batch   30/33 - Loss:  0.295, Seconds: 0.39\n",
            "Epoch  15/30 Batch   32/33 - Loss:  1.052, Seconds: 0.57\n",
            "Average loss for this update: 0.569\n",
            "New Record!\n",
            "Epoch  16/30 Batch    2/33 - Loss:  0.862, Seconds: 0.18\n",
            "Epoch  16/30 Batch    4/33 - Loss:  0.322, Seconds: 0.18\n",
            "Epoch  16/30 Batch    6/33 - Loss:  0.272, Seconds: 0.20\n",
            "Epoch  16/30 Batch    8/33 - Loss:  0.121, Seconds: 0.20\n",
            "Epoch  16/30 Batch   10/33 - Loss:  0.488, Seconds: 0.22\n",
            "Epoch  16/30 Batch   12/33 - Loss:  0.085, Seconds: 0.25\n",
            "Epoch  16/30 Batch   14/33 - Loss:  0.812, Seconds: 0.30\n",
            "Epoch  16/30 Batch   16/33 - Loss:  0.604, Seconds: 0.31\n",
            "Epoch  16/30 Batch   18/33 - Loss:  0.414, Seconds: 0.35\n",
            "Epoch  16/30 Batch   20/33 - Loss:  0.215, Seconds: 0.36\n",
            "Epoch  16/30 Batch   22/33 - Loss:  0.399, Seconds: 0.35\n",
            "Epoch  16/30 Batch   24/33 - Loss:  0.526, Seconds: 0.41\n",
            "Epoch  16/30 Batch   26/33 - Loss:  0.502, Seconds: 0.41\n",
            "Epoch  16/30 Batch   28/33 - Loss:  0.669, Seconds: 0.45\n",
            "Epoch  16/30 Batch   30/33 - Loss:  0.378, Seconds: 0.41\n",
            "Epoch  16/30 Batch   32/33 - Loss:  0.889, Seconds: 0.58\n",
            "Average loss for this update: 0.472\n",
            "New Record!\n",
            "Epoch  17/30 Batch    2/33 - Loss:  0.851, Seconds: 0.25\n",
            "Epoch  17/30 Batch    4/33 - Loss:  0.141, Seconds: 0.25\n",
            "Epoch  17/30 Batch    6/33 - Loss:  0.245, Seconds: 0.28\n",
            "Epoch  17/30 Batch    8/33 - Loss:  0.088, Seconds: 0.26\n",
            "Epoch  17/30 Batch   10/33 - Loss:  0.603, Seconds: 0.28\n",
            "Epoch  17/30 Batch   12/33 - Loss:  0.058, Seconds: 0.32\n",
            "Epoch  17/30 Batch   14/33 - Loss:  0.462, Seconds: 0.23\n",
            "Epoch  17/30 Batch   16/33 - Loss:  0.484, Seconds: 0.33\n",
            "Epoch  17/30 Batch   18/33 - Loss:  0.336, Seconds: 0.36\n",
            "Epoch  17/30 Batch   20/33 - Loss:  0.104, Seconds: 0.34\n",
            "Epoch  17/30 Batch   22/33 - Loss:  0.225, Seconds: 0.35\n",
            "Epoch  17/30 Batch   24/33 - Loss:  0.145, Seconds: 0.44\n",
            "Epoch  17/30 Batch   26/33 - Loss:  0.492, Seconds: 0.33\n",
            "Epoch  17/30 Batch   28/33 - Loss:  0.662, Seconds: 0.38\n",
            "Epoch  17/30 Batch   30/33 - Loss:  0.435, Seconds: 0.50\n",
            "Epoch  17/30 Batch   32/33 - Loss:  0.789, Seconds: 0.47\n",
            "Average loss for this update: 0.382\n",
            "New Record!\n",
            "Epoch  18/30 Batch    2/33 - Loss:  0.970, Seconds: 0.19\n",
            "Epoch  18/30 Batch    4/33 - Loss:  0.097, Seconds: 0.17\n",
            "Epoch  18/30 Batch    6/33 - Loss:  0.323, Seconds: 0.22\n",
            "Epoch  18/30 Batch    8/33 - Loss:  0.093, Seconds: 0.25\n",
            "Epoch  18/30 Batch   10/33 - Loss:  0.343, Seconds: 0.29\n",
            "Epoch  18/30 Batch   12/33 - Loss:  0.064, Seconds: 0.28\n",
            "Epoch  18/30 Batch   14/33 - Loss:  0.332, Seconds: 0.27\n",
            "Epoch  18/30 Batch   16/33 - Loss:  0.592, Seconds: 0.26\n",
            "Epoch  18/30 Batch   18/33 - Loss:  0.228, Seconds: 0.28\n",
            "Epoch  18/30 Batch   20/33 - Loss:  0.032, Seconds: 0.31\n",
            "Epoch  18/30 Batch   22/33 - Loss:  0.228, Seconds: 0.30\n",
            "Epoch  18/30 Batch   24/33 - Loss:  0.130, Seconds: 0.31\n",
            "Epoch  18/30 Batch   26/33 - Loss:  0.322, Seconds: 0.34\n",
            "Epoch  18/30 Batch   28/33 - Loss:  0.624, Seconds: 0.41\n",
            "Epoch  18/30 Batch   30/33 - Loss:  0.245, Seconds: 0.43\n",
            "Epoch  18/30 Batch   32/33 - Loss:  0.477, Seconds: 0.56\n",
            "Average loss for this update: 0.319\n",
            "New Record!\n",
            "Epoch  19/30 Batch    2/33 - Loss:  0.512, Seconds: 0.20\n",
            "Epoch  19/30 Batch    4/33 - Loss:  0.085, Seconds: 0.24\n",
            "Epoch  19/30 Batch    6/33 - Loss:  0.304, Seconds: 0.27\n",
            "Epoch  19/30 Batch    8/33 - Loss:  0.050, Seconds: 0.29\n",
            "Epoch  19/30 Batch   10/33 - Loss:  0.261, Seconds: 0.23\n",
            "Epoch  19/30 Batch   12/33 - Loss:  0.039, Seconds: 0.29\n",
            "Epoch  19/30 Batch   14/33 - Loss:  0.448, Seconds: 0.23\n",
            "Epoch  19/30 Batch   16/33 - Loss:  0.278, Seconds: 0.29\n",
            "Epoch  19/30 Batch   18/33 - Loss:  0.591, Seconds: 0.27\n",
            "Epoch  19/30 Batch   20/33 - Loss:  0.034, Seconds: 0.31\n",
            "Epoch  19/30 Batch   22/33 - Loss:  0.224, Seconds: 0.28\n",
            "Epoch  19/30 Batch   24/33 - Loss:  0.160, Seconds: 0.42\n",
            "Epoch  19/30 Batch   26/33 - Loss:  0.302, Seconds: 0.34\n",
            "Epoch  19/30 Batch   28/33 - Loss:  0.618, Seconds: 0.42\n",
            "Epoch  19/30 Batch   30/33 - Loss:  0.259, Seconds: 0.48\n",
            "Epoch  19/30 Batch   32/33 - Loss:  0.575, Seconds: 0.50\n",
            "Average loss for this update: 0.296\n",
            "New Record!\n",
            "Epoch  20/30 Batch    2/33 - Loss:  0.571, Seconds: 0.22\n",
            "Epoch  20/30 Batch    4/33 - Loss:  0.139, Seconds: 0.17\n",
            "Epoch  20/30 Batch    6/33 - Loss:  0.266, Seconds: 0.22\n",
            "Epoch  20/30 Batch    8/33 - Loss:  0.032, Seconds: 0.28\n",
            "Epoch  20/30 Batch   10/33 - Loss:  0.282, Seconds: 0.29\n",
            "Epoch  20/30 Batch   12/33 - Loss:  0.035, Seconds: 0.23\n",
            "Epoch  20/30 Batch   14/33 - Loss:  0.379, Seconds: 0.22\n",
            "Epoch  20/30 Batch   16/33 - Loss:  0.371, Seconds: 0.33\n",
            "Epoch  20/30 Batch   18/33 - Loss:  0.524, Seconds: 0.31\n",
            "Epoch  20/30 Batch   20/33 - Loss:  0.045, Seconds: 0.27\n",
            "Epoch  20/30 Batch   22/33 - Loss:  0.214, Seconds: 0.29\n",
            "Epoch  20/30 Batch   24/33 - Loss:  0.148, Seconds: 0.39\n",
            "Epoch  20/30 Batch   26/33 - Loss:  0.386, Seconds: 0.33\n",
            "Epoch  20/30 Batch   28/33 - Loss:  0.643, Seconds: 0.42\n",
            "Epoch  20/30 Batch   30/33 - Loss:  0.160, Seconds: 0.42\n",
            "Epoch  20/30 Batch   32/33 - Loss:  0.618, Seconds: 0.46\n",
            "Average loss for this update: 0.301\n",
            "No Improvement.\n",
            "Epoch  21/30 Batch    2/33 - Loss:  0.311, Seconds: 0.24\n",
            "Epoch  21/30 Batch    4/33 - Loss:  0.072, Seconds: 0.17\n",
            "Epoch  21/30 Batch    6/33 - Loss:  0.247, Seconds: 0.21\n",
            "Epoch  21/30 Batch    8/33 - Loss:  0.049, Seconds: 0.22\n",
            "Epoch  21/30 Batch   10/33 - Loss:  0.270, Seconds: 0.29\n",
            "Epoch  21/30 Batch   12/33 - Loss:  0.031, Seconds: 0.22\n",
            "Epoch  21/30 Batch   14/33 - Loss:  0.261, Seconds: 0.25\n",
            "Epoch  21/30 Batch   16/33 - Loss:  0.226, Seconds: 0.25\n",
            "Epoch  21/30 Batch   18/33 - Loss:  0.245, Seconds: 0.27\n",
            "Epoch  21/30 Batch   20/33 - Loss:  0.099, Seconds: 0.27\n",
            "Epoch  21/30 Batch   22/33 - Loss:  0.111, Seconds: 0.38\n",
            "Epoch  21/30 Batch   24/33 - Loss:  0.093, Seconds: 0.39\n",
            "Epoch  21/30 Batch   26/33 - Loss:  0.302, Seconds: 0.33\n",
            "Epoch  21/30 Batch   28/33 - Loss:  0.411, Seconds: 0.38\n",
            "Epoch  21/30 Batch   30/33 - Loss:  0.256, Seconds: 0.41\n",
            "Epoch  21/30 Batch   32/33 - Loss:  0.541, Seconds: 0.58\n",
            "Average loss for this update: 0.22\n",
            "New Record!\n",
            "Epoch  22/30 Batch    2/33 - Loss:  0.325, Seconds: 0.24\n",
            "Epoch  22/30 Batch    4/33 - Loss:  0.029, Seconds: 0.22\n",
            "Epoch  22/30 Batch    6/33 - Loss:  0.081, Seconds: 0.26\n",
            "Epoch  22/30 Batch    8/33 - Loss:  0.065, Seconds: 0.31\n",
            "Epoch  22/30 Batch   10/33 - Loss:  0.085, Seconds: 0.23\n",
            "Epoch  22/30 Batch   12/33 - Loss:  0.037, Seconds: 0.22\n",
            "Epoch  22/30 Batch   14/33 - Loss:  0.230, Seconds: 0.29\n",
            "Epoch  22/30 Batch   16/33 - Loss:  0.213, Seconds: 0.36\n",
            "Epoch  22/30 Batch   18/33 - Loss:  0.115, Seconds: 0.33\n",
            "Epoch  22/30 Batch   20/33 - Loss:  0.109, Seconds: 0.27\n",
            "Epoch  22/30 Batch   22/33 - Loss:  0.051, Seconds: 0.38\n",
            "Epoch  22/30 Batch   24/33 - Loss:  0.072, Seconds: 0.39\n",
            "Epoch  22/30 Batch   26/33 - Loss:  0.361, Seconds: 0.33\n",
            "Epoch  22/30 Batch   28/33 - Loss:  0.351, Seconds: 0.53\n",
            "Epoch  22/30 Batch   30/33 - Loss:  0.084, Seconds: 0.47\n",
            "Epoch  22/30 Batch   32/33 - Loss:  0.447, Seconds: 0.62\n",
            "Average loss for this update: 0.166\n",
            "New Record!\n",
            "Epoch  23/30 Batch    2/33 - Loss:  0.349, Seconds: 0.25\n",
            "Epoch  23/30 Batch    4/33 - Loss:  0.037, Seconds: 0.19\n",
            "Epoch  23/30 Batch    6/33 - Loss:  0.090, Seconds: 0.21\n",
            "Epoch  23/30 Batch    8/33 - Loss:  0.081, Seconds: 0.22\n",
            "Epoch  23/30 Batch   10/33 - Loss:  0.069, Seconds: 0.23\n",
            "Epoch  23/30 Batch   12/33 - Loss:  0.019, Seconds: 0.23\n",
            "Epoch  23/30 Batch   14/33 - Loss:  0.407, Seconds: 0.23\n",
            "Epoch  23/30 Batch   16/33 - Loss:  0.181, Seconds: 0.33\n",
            "Epoch  23/30 Batch   18/33 - Loss:  0.090, Seconds: 0.34\n",
            "Epoch  23/30 Batch   20/33 - Loss:  0.119, Seconds: 0.30\n",
            "Epoch  23/30 Batch   22/33 - Loss:  0.061, Seconds: 0.26\n",
            "Epoch  23/30 Batch   24/33 - Loss:  0.112, Seconds: 0.39\n",
            "Epoch  23/30 Batch   26/33 - Loss:  0.248, Seconds: 0.33\n",
            "Epoch  23/30 Batch   28/33 - Loss:  0.302, Seconds: 0.37\n",
            "Epoch  23/30 Batch   30/33 - Loss:  0.036, Seconds: 0.48\n",
            "Epoch  23/30 Batch   32/33 - Loss:  0.306, Seconds: 0.49\n",
            "Average loss for this update: 0.157\n",
            "New Record!\n",
            "Epoch  24/30 Batch    2/33 - Loss:  0.231, Seconds: 0.23\n",
            "Epoch  24/30 Batch    4/33 - Loss:  0.096, Seconds: 0.24\n",
            "Epoch  24/30 Batch    6/33 - Loss:  0.123, Seconds: 0.28\n",
            "Epoch  24/30 Batch    8/33 - Loss:  0.026, Seconds: 0.19\n",
            "Epoch  24/30 Batch   10/33 - Loss:  0.074, Seconds: 0.28\n",
            "Epoch  24/30 Batch   12/33 - Loss:  0.023, Seconds: 0.25\n",
            "Epoch  24/30 Batch   14/33 - Loss:  0.189, Seconds: 0.23\n",
            "Epoch  24/30 Batch   16/33 - Loss:  0.087, Seconds: 0.25\n",
            "Epoch  24/30 Batch   18/33 - Loss:  0.047, Seconds: 0.28\n",
            "Epoch  24/30 Batch   20/33 - Loss:  0.021, Seconds: 0.28\n",
            "Epoch  24/30 Batch   22/33 - Loss:  0.045, Seconds: 0.33\n",
            "Epoch  24/30 Batch   24/33 - Loss:  0.069, Seconds: 0.35\n",
            "Epoch  24/30 Batch   26/33 - Loss:  0.213, Seconds: 0.32\n",
            "Epoch  24/30 Batch   28/33 - Loss:  0.326, Seconds: 0.40\n",
            "Epoch  24/30 Batch   30/33 - Loss:  0.032, Seconds: 0.40\n",
            "Epoch  24/30 Batch   32/33 - Loss:  0.305, Seconds: 0.50\n",
            "Average loss for this update: 0.119\n",
            "New Record!\n",
            "Epoch  25/30 Batch    2/33 - Loss:  0.116, Seconds: 0.21\n",
            "Epoch  25/30 Batch    4/33 - Loss:  0.021, Seconds: 0.16\n",
            "Epoch  25/30 Batch    6/33 - Loss:  0.086, Seconds: 0.20\n",
            "Epoch  25/30 Batch    8/33 - Loss:  0.019, Seconds: 0.20\n",
            "Epoch  25/30 Batch   10/33 - Loss:  0.051, Seconds: 0.26\n",
            "Epoch  25/30 Batch   12/33 - Loss:  0.022, Seconds: 0.24\n",
            "Epoch  25/30 Batch   14/33 - Loss:  0.061, Seconds: 0.31\n",
            "Epoch  25/30 Batch   16/33 - Loss:  0.067, Seconds: 0.32\n",
            "Epoch  25/30 Batch   18/33 - Loss:  0.043, Seconds: 0.30\n",
            "Epoch  25/30 Batch   20/33 - Loss:  0.017, Seconds: 0.34\n",
            "Epoch  25/30 Batch   22/33 - Loss:  0.043, Seconds: 0.35\n",
            "Epoch  25/30 Batch   24/33 - Loss:  0.032, Seconds: 0.37\n",
            "Epoch  25/30 Batch   26/33 - Loss:  0.269, Seconds: 0.41\n",
            "Epoch  25/30 Batch   28/33 - Loss:  0.268, Seconds: 0.47\n",
            "Epoch  25/30 Batch   30/33 - Loss:  0.022, Seconds: 0.49\n",
            "Epoch  25/30 Batch   32/33 - Loss:  0.295, Seconds: 0.53\n",
            "Average loss for this update: 0.089\n",
            "New Record!\n",
            "Epoch  26/30 Batch    2/33 - Loss:  0.079, Seconds: 0.19\n",
            "Epoch  26/30 Batch    4/33 - Loss:  0.028, Seconds: 0.16\n",
            "Epoch  26/30 Batch    6/33 - Loss:  0.068, Seconds: 0.23\n",
            "Epoch  26/30 Batch    8/33 - Loss:  0.030, Seconds: 0.22\n",
            "Epoch  26/30 Batch   10/33 - Loss:  0.070, Seconds: 0.22\n",
            "Epoch  26/30 Batch   12/33 - Loss:  0.016, Seconds: 0.22\n",
            "Epoch  26/30 Batch   14/33 - Loss:  0.073, Seconds: 0.29\n",
            "Epoch  26/30 Batch   16/33 - Loss:  0.076, Seconds: 0.34\n",
            "Epoch  26/30 Batch   18/33 - Loss:  0.030, Seconds: 0.32\n",
            "Epoch  26/30 Batch   20/33 - Loss:  0.013, Seconds: 0.26\n",
            "Epoch  26/30 Batch   22/33 - Loss:  0.021, Seconds: 0.34\n",
            "Epoch  26/30 Batch   24/33 - Loss:  0.023, Seconds: 0.37\n",
            "Epoch  26/30 Batch   26/33 - Loss:  0.283, Seconds: 0.33\n",
            "Epoch  26/30 Batch   28/33 - Loss:  0.286, Seconds: 0.47\n",
            "Epoch  26/30 Batch   30/33 - Loss:  0.027, Seconds: 0.43\n",
            "Epoch  26/30 Batch   32/33 - Loss:  0.354, Seconds: 0.52\n",
            "Average loss for this update: 0.092\n",
            "No Improvement.\n",
            "Epoch  27/30 Batch    2/33 - Loss:  0.064, Seconds: 0.25\n",
            "Epoch  27/30 Batch    4/33 - Loss:  0.038, Seconds: 0.17\n",
            "Epoch  27/30 Batch    6/33 - Loss:  0.033, Seconds: 0.21\n",
            "Epoch  27/30 Batch    8/33 - Loss:  0.015, Seconds: 0.22\n",
            "Epoch  27/30 Batch   10/33 - Loss:  0.087, Seconds: 0.30\n",
            "Epoch  27/30 Batch   12/33 - Loss:  0.011, Seconds: 0.25\n",
            "Epoch  27/30 Batch   14/33 - Loss:  0.112, Seconds: 0.31\n",
            "Epoch  27/30 Batch   16/33 - Loss:  0.151, Seconds: 0.26\n",
            "Epoch  27/30 Batch   18/33 - Loss:  0.033, Seconds: 0.31\n",
            "Epoch  27/30 Batch   20/33 - Loss:  0.012, Seconds: 0.27\n",
            "Epoch  27/30 Batch   22/33 - Loss:  0.027, Seconds: 0.29\n",
            "Epoch  27/30 Batch   24/33 - Loss:  0.019, Seconds: 0.36\n",
            "Epoch  27/30 Batch   26/33 - Loss:  0.137, Seconds: 0.44\n",
            "Epoch  27/30 Batch   28/33 - Loss:  0.195, Seconds: 0.41\n",
            "Epoch  27/30 Batch   30/33 - Loss:  0.030, Seconds: 0.51\n",
            "Epoch  27/30 Batch   32/33 - Loss:  0.326, Seconds: 0.48\n",
            "Average loss for this update: 0.081\n",
            "New Record!\n",
            "Epoch  28/30 Batch    2/33 - Loss:  0.056, Seconds: 0.24\n",
            "Epoch  28/30 Batch    4/33 - Loss:  0.026, Seconds: 0.18\n",
            "Epoch  28/30 Batch    6/33 - Loss:  0.023, Seconds: 0.20\n",
            "Epoch  28/30 Batch    8/33 - Loss:  0.013, Seconds: 0.20\n",
            "Epoch  28/30 Batch   10/33 - Loss:  0.128, Seconds: 0.23\n",
            "Epoch  28/30 Batch   12/33 - Loss:  0.017, Seconds: 0.22\n",
            "Epoch  28/30 Batch   14/33 - Loss:  0.067, Seconds: 0.25\n",
            "Epoch  28/30 Batch   16/33 - Loss:  0.083, Seconds: 0.27\n",
            "Epoch  28/30 Batch   18/33 - Loss:  0.028, Seconds: 0.34\n",
            "Epoch  28/30 Batch   20/33 - Loss:  0.010, Seconds: 0.32\n",
            "Epoch  28/30 Batch   22/33 - Loss:  0.026, Seconds: 0.37\n",
            "Epoch  28/30 Batch   24/33 - Loss:  0.021, Seconds: 0.39\n",
            "Epoch  28/30 Batch   26/33 - Loss:  0.128, Seconds: 0.41\n",
            "Epoch  28/30 Batch   28/33 - Loss:  0.241, Seconds: 0.47\n",
            "Epoch  28/30 Batch   30/33 - Loss:  0.021, Seconds: 0.40\n",
            "Epoch  28/30 Batch   32/33 - Loss:  0.298, Seconds: 0.49\n",
            "Average loss for this update: 0.074\n",
            "New Record!\n",
            "Epoch  29/30 Batch    2/33 - Loss:  0.049, Seconds: 0.24\n",
            "Epoch  29/30 Batch    4/33 - Loss:  0.015, Seconds: 0.21\n",
            "Epoch  29/30 Batch    6/33 - Loss:  0.029, Seconds: 0.24\n",
            "Epoch  29/30 Batch    8/33 - Loss:  0.013, Seconds: 0.27\n",
            "Epoch  29/30 Batch   10/33 - Loss:  0.050, Seconds: 0.29\n",
            "Epoch  29/30 Batch   12/33 - Loss:  0.013, Seconds: 0.24\n",
            "Epoch  29/30 Batch   14/33 - Loss:  0.083, Seconds: 0.23\n",
            "Epoch  29/30 Batch   16/33 - Loss:  0.053, Seconds: 0.27\n",
            "Epoch  29/30 Batch   18/33 - Loss:  0.021, Seconds: 0.36\n",
            "Epoch  29/30 Batch   20/33 - Loss:  0.011, Seconds: 0.34\n",
            "Epoch  29/30 Batch   22/33 - Loss:  0.023, Seconds: 0.35\n",
            "Epoch  29/30 Batch   24/33 - Loss:  0.021, Seconds: 0.37\n",
            "Epoch  29/30 Batch   26/33 - Loss:  0.120, Seconds: 0.35\n",
            "Epoch  29/30 Batch   28/33 - Loss:  0.250, Seconds: 0.37\n",
            "Epoch  29/30 Batch   30/33 - Loss:  0.017, Seconds: 0.44\n",
            "Epoch  29/30 Batch   32/33 - Loss:  0.207, Seconds: 0.52\n",
            "Average loss for this update: 0.061\n",
            "New Record!\n",
            "Epoch  30/30 Batch    2/33 - Loss:  0.062, Seconds: 0.24\n",
            "Epoch  30/30 Batch    4/33 - Loss:  0.015, Seconds: 0.22\n",
            "Epoch  30/30 Batch    6/33 - Loss:  0.019, Seconds: 0.19\n",
            "Epoch  30/30 Batch    8/33 - Loss:  0.011, Seconds: 0.20\n",
            "Epoch  30/30 Batch   10/33 - Loss:  0.036, Seconds: 0.25\n",
            "Epoch  30/30 Batch   12/33 - Loss:  0.010, Seconds: 0.29\n",
            "Epoch  30/30 Batch   14/33 - Loss:  0.098, Seconds: 0.26\n",
            "Epoch  30/30 Batch   16/33 - Loss:  0.034, Seconds: 0.26\n",
            "Epoch  30/30 Batch   18/33 - Loss:  0.021, Seconds: 0.32\n",
            "Epoch  30/30 Batch   20/33 - Loss:  0.010, Seconds: 0.28\n",
            "Epoch  30/30 Batch   22/33 - Loss:  0.020, Seconds: 0.27\n",
            "Epoch  30/30 Batch   24/33 - Loss:  0.016, Seconds: 0.34\n",
            "Epoch  30/30 Batch   26/33 - Loss:  0.071, Seconds: 0.43\n",
            "Epoch  30/30 Batch   28/33 - Loss:  0.159, Seconds: 0.37\n",
            "Epoch  30/30 Batch   30/33 - Loss:  0.014, Seconds: 0.40\n",
            "Epoch  30/30 Batch   32/33 - Loss:  0.191, Seconds: 0.58\n",
            "Average loss for this update: 0.049\n",
            "New Record!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc9K0Z6TDKds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_to_seq(text):\n",
        "    '''Prepare the text for the model'''\n",
        "    text = clean_text(text)\n",
        "    \n",
        "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ah-1f5PDKdx",
        "colab_type": "code",
        "outputId": "128c0ee2-a3f3-4773-a833-514b5dd9a268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "random = np.random.randint(0,len(clean_texts))\n",
        "input_sentence = clean_texts[random]\n",
        "text = text_to_seq(clean_texts[random])\n",
        "\n",
        "checkpoint = \"model1.ckpt\"\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    loader.restore(sess, checkpoint)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "    \n",
        "    #Multiply by batch_size to match the model's input parameters\n",
        "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                      summary_length: [np.random.randint(5,8)], \n",
        "                                      text_length: [len(text)]*batch_size,\n",
        "                                      keep_prob: 1.0})[0] \n",
        "\n",
        "# Remove the padding from the tweet\n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "\n",
        "print('Original Text:', df.Text[random])\n",
        "print('Original summary:', df.Summary[random])#clean_summaries[random]\n",
        "\n",
        "print('\\nText')\n",
        "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "print('\\nSummary')\n",
        "print('Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from model1.ckpt\n",
            "Original Text: একদিন নিজের একটা পরিচয় হবে,একটা ঘর হবে,একান্তই নিজের,নিজের অনেক টাকা হবে........আমি কেন্দ্রিক সব কিছু হবে কোনো এক দিন.... সেই এক দিনের প্রতিক্ষায় আছি.....হয়তো পেরিয়ে আসা বয়স টা থাকবেনা,কিন্তু থেকে যাবে মনের চাপা কষ্ট গুলি,আবেগ গুলি, না বলতে পারা কথাগুলি...........তবুও সেই দিন টার প্রতিক্ষা করি..... একটা দিন..............আম্মা তুমি শুধু তোমার মেয়েকে একটু দোয়া দিও বেশি করে....তোমার দোয়া তোমার মেয়েকে ওই দিনটার কাছে পৌছঁতে সাহায্য করবে.......\n",
            "Original summary: একদিন নিজের একটা পরিচয় হবে। \n",
            "\n",
            "Text\n",
            "  Input Words: একদিন নিজের একটা পরিচয় হবে একটা ঘর হবে একান্তই নিজের নিজের অনেক টাকা হবে আমি কেন্দ্রিক সব কিছু হবে কোনো এক দিন সেই এক দিনের প্রতিক্ষায় আছি হয়তো পেরিয়ে আসা বয়স টা থাকবেনা কিন্তু থেকে যাবে মনের চাপা কষ্ট গুলি আবেগ গুলি না বলতে পারা কথাগুলি তবুও সেই দিন টার প্রতিক্ষা করি একটা দিন আম্মা তুমি শুধু তোমার মেয়েকে একটু দোয়া দিও বেশি করে তোমার দোয়া তোমার মেয়েকে ওই দিনটার কাছে <UNK> সাহায্য করবে\n",
            "\n",
            "Summary\n",
            "Response Words: একদিন নিজের একটা পরিচয় হবে একটা একটা\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bstBLCLiDKd6",
        "colab_type": "code",
        "outputId": "0207b4f9-064a-4f1d-99ce-e413910d5a68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "input_sentence = input()\n",
        "text = text_to_seq(input_sentence)\n",
        "\n",
        "checkpoint = \"model1.ckpt\"\n",
        "\n",
        "loaded_graph = tf.Graph()\n",
        "with tf.Session(graph=loaded_graph) as sess:\n",
        "    # Load saved model\n",
        "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    loader.restore(sess, checkpoint)\n",
        "\n",
        "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
        "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
        "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
        "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
        "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "    \n",
        "    #Multiply by batch_size to match the model's input parameters\n",
        "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
        "                                      summary_length: [np.random.randint(5,8)], \n",
        "                                      text_length: [len(text)]*batch_size,\n",
        "                                      keep_prob: 1.0})[0] \n",
        "\n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "\n",
        "print('\\nText')\n",
        "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
        "\n",
        "print('\\nSummary')\n",
        "print('Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "যানজটে জান শেষ! ট্রাফিক জ্যাম আমাদের জীবনের অবিচ্ছেদ্য একটি বিষয়ে পরিণত হয়েছে। অফিস, বিশ্ববিদ্যালয় বা যেকোনো কাজের ক্ষেত্রে আমাদের চলতিপথে যানজটের সম্মুখীন হতে হবে, এটাই স্বাভাবিক, বরং না হওয়াটাই অস্বাভাবিক মনে হয়।\n",
            "INFO:tensorflow:Restoring parameters from model1.ckpt\n",
            "\n",
            "Text\n",
            "  Input Words: যানজটে জান শেষ ট্রাফিক জ্যাম আমাদের জীবনের অবিচ্ছেদ্য একটি বিষয়ে পরিণত হয়েছে অফিস বিশ্ববিদ্যালয় বা যেকোনো কাজের ক্ষেত্রে আমাদের চলতিপথে যানজটের সম্মুখীন হতে হবে এটাই স্বাভাবিক বরং না হওয়াটাই অস্বাভাবিক মনে হয়\n",
            "\n",
            "Summary\n",
            "Response Words: ট্রাফিক জ্যাম আমাদের জীবনের অবিচ্ছেদ্য একটি\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdwoSNxSDKd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}